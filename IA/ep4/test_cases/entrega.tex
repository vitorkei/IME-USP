\documentclass{article}

\usepackage[brazilian]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{MnSymbol}
\usepackage{wasysym}
\usepackage{mdframed}
\usepackage[a4paper, total={6in, 10.5in}]{geometry}

\title{\textbf{MAC425/5739 - Inteligência Artificial}}
\author{\textbf{Vítor Kei Taira Tamada - 8516250}}
\date{\textbf{Exercício-programa 4 - Relatório}}

\begin{document}

\maketitle

\textbf{\LARGE{Relatório}}

\quad \textbf{(a)}

\begin{center}
	\begin{tabular}{| c  c | c | c | c |}
		\hline
			& Teste & wine & car & ionosphere \\ 
			& & & & \\
			Métrica & maxdepth & & & \\ \hline
			
			& N & OK & FAIL & OK \\
			& 1 & FAIL & FAIL & OK \\
			Entropy & 2 & OK & FAIL & OK \\
			& 4 & OK & FAIL & OK \\
			& 5 & FAIL & FAIL & OK \\ \hline
			
			& N & OK & FAIL & OK \\
			& 1 & FAIL & FAIL & OK \\
			Gini & 2 & OK & FAIL & OK \\
			& 4 & OK & FAIL & OK \\
			& 5 & FAIL & FAIL & OK \\ \hline
			
			& N & FAIL & FAIL & FAIL \\
			& 1 & FAIL & FAIL & FAIL \\
			Error & 2 & FAIL & FAIL & FAIL \\
			& 4 & FAIL & FAIL & FAIL \\
			& 5 & FAIL & FAIL & FAIL \\ \hline
	\end{tabular}
	
	N(t) = valor padrão de maxdepth para o teste t
	
	N(wine) = 3 | N(car) = 4 | N(ionosphere) = 1
	
	\bigskip
	FAIL significa que a mensagem "Most frequent classifier is better" foi retornada para o teste
\end{center}

\bigskip
\quad \textbf{(b)} Tanto o critério de entropia (\textit{entropy}) quanto o Gini tem resultados semelhantes. Isso se deve ao fato de ambos medirem algo parecido: enquanto Gini mede a probabilidade de um exemplo aleatório ser classificado corretamente se a classe do subconjunto a ser atribuída for escolhida aleatóriamente, a entropia mede o ganho de informação. Entretanto, há méritos em se utilizar Gini ao invés da entropia: por conta da forma como se cálcula cada um deles, a entropia é naturalmente mais lenta devido à operação logarítmica.

\qquad A divisão baseada em erro de classificação, por outro lado, mede o erro de classificação de um nó, como o nome diz. Como é possível ver na tabela, este critério tende a errar mais, uma vez que as divisões de nós feitas são encerradas mais rapidamente em comparação com Gini e entropia.

\bigskip
\quad \textbf{(c)}\textbf{ Questões téoricas}

\bigskip
\quad \textbf{\large{Questão 1 - Métricas}}

\qquad R) Muito importante dado que diferentes algoritmos utilizam diferentes métricas para determinar qual o melhor atributo para dividir a árvore em cada nó: enquanto Gini é utilizado em algoritmos de classificação e de árvores de regressão (\textit{CART}), entropia/ganho de informação é melhor para algoritmos de geração de árvores.

\bigskip
\quad\textbf{\large{Questão 2 - Condição de parada}}

\qquad R) Verificar se o conjunto de dados sendo verificados é puro, ou seja, se todas as entradas tem a mesma classificação (\textit{label}), se todas as entradas do conjunto de dados são iguais e se o conjunto de dados é vazio.

\bigskip
\quad\textbf{\large{Questão 3 - Acurácia}}

\qquad R) Um dos motivos é que por mais que a árvore tenha um \texttt{self.maxdepth} maior, os nós folhas são alcançados antes mesmo de chegar nessa profundidade máxima; ou seja, a acurácia não aumenta nem diminui.

\qquad Por outro lado, se os dados de treinamento possuirem muitos atributos, o aumento da profundidade faz com que muitos deles sejam analisados - mais do que o necessário. Isso resulta no problema de \textit{overfitting}, que reduz a acurácia da árvore de classificação.

\bigskip
\quad\textbf{\large{Questão 4 - Comparação}}

\qquad R) Quando a árvore de decisão formada ou a sua construção for complexa ou custosa demais, ou quando o conjunto de dados de treinamento ainda precisa ser tratado (eliminação de entradas duplicadas ainda é necessária por exemplo). Ou seja, quando não vale a pena construir a árvore de decisão.

\bigskip
\quad \textbf{(d)}

\end{document}
